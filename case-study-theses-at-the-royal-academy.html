<section data-type="chapter">
<h1>Theses at the Royal Academy of Arts, The Hague</h1>

<figure><img alt="Theses at the Royal Academy of Arts, The Hague" src="images/kabk.png" /></figure>

<p>The Royal Academy of Arts in The Hague, The Netherlands runs a renowned Graphic Design Bachelor program. In recent years head teachers Roosje Klap en Niels Schrader have reinvigorated the program with a focus on social responsibility and the appropriation of digital technologies.</p>

<p>The thesis has been part of the curriculum for many years. It serves as a possibility for the student to research into a specific area of interest and will help underly the student’s final exam project. In contrast to theses and papers written at the universities, the students are are expected to engage in the design of their thesis. Form and content are decidedly mixed.</p>

<p>With screen (and more specifically, the web) obtaining the status of the medium through which all other mediums are represented, it makes sense for students to re-imagine their thesis for this medium. Starting in the academic year 2014/2015 we ask students to create their thesis for the screen. More specifically, we presented the students not with a screen-only model, but a screen-centric model. The students are free to create a print version of their thesis, but are encouraged to take the version conceived for the screen as the departure point.</p>

<p>For this project, the last think we want to do is give the students a template. At the same time, we do need to think the process through, because, with a group of 50 students, we need to be able to follow the process of all students, be able to give feedback. Also, we are legally required to archive all theses. Finally, we want to publish the theses online in an accessible and durable way.</p>

<p>The proposed solution is to have students work with what is the raw material of the internet: the web standards HTML, CSS and JavaScript. In most contemporary web systems these files are generated by a dynamic system. We choose to go Back to the Future, and write websites like it’s 1999: in actual separate HTML files. The files are exchanged archived on a code sharing website GitHub. The same website also makes them available through the web. Finally, the metadata from the theses are analysed, and a homepage is created from them.</p>

<p>Students coded their own design. As part of the curriculum. The decision to use web standards HTML, JavaScript and CSS made development more simple as it removed a layer of choice.</p>

<section data-type="sect1">
<h1>The paradox of choice</h1>

<p class="p1">The landscape of web technology is varied and ever changing. Creating an advanced web project might include choosing a server side programming language (Python, PHP, Node.js) that talks to a database of choice, then creating the visual and interaction design on top of a foundation of existing front-end ‘frameworks’ and ‘pre-processors’ . The amount of choices to make is staggering, and the complexities of such systems high. For the project at the KABK we have foregone these technologies, for a more simple, back to basic approach: writing HTML pages.</p>

<p class="p1">The first web-sites were exactly this: ‘hand-crafted HTML’. many small sites were created as a series of HTML pages, with occasional updates (the person designing the site might then charge for each update!). This did not mean coding was necessary: tools like Adobe Dreamweaver provided a visual view and a code view. The democratisation of Content Management Systems like WordPress and Joomla changed the equation. In these systems, a general design is encoded into a template, and the contents for individual pages are stored in a database that is easily editable by the user.&nbsp;For clients this saves time and money. The downside is that a content management system requires shoehorning every page into templates: these early HTML pages offered much more freedom in this respect, as potentially every page could be modified and changed to the designer’s whims.</p>

<p class="p1">An ideal system would undoubtly combine elements of both paradigms: easy customisation and user-friendly content management. But with the design students, we choose to go Back to the Future, and write websites like it ’s 1999: in HTML files. Here are the reasons:</p>

<ul>
	<li class="p1">The contents and design of the thesis are supposed to be intimately related. Traditional web systems HTML, CSS and JavaScript are the language of web design. To be able to directly edit the HTML of the pages instead of generating them through a system, gives the full control needed in this situation.</li>
	<li class="p1">The advantages of Content Management Systems become apparent for larger collections of texts that need to be similarly laid out. The thesis is one text, and not a very long text at that. There is little efficiency to be gained from creating a template.</li>
	<li class="p1">Graphic designers can be expected to know the languages of the web. At least, in the programme at the KABK they are. So even if the editing of text files is not a very accessible method for the larger public, it is perfectly acceptable for the intended audience.</li>
</ul>

<section data-type="sect2">
<h2>Working with Github</h2>

<figure><img alt="Working with Github" src="images/kabk_github.png" /></figure>

<p>Github is code hosting site that focuses on facilitating the collaboration between developers. It is built on the Open Source software Git. Each contributor works on their own computer, on their own version of the files. They can then use Git to merge their changes with those of others.</p>

<p>Github also offers the possibilty to publish files to the internet. This feature is usually used to publish the documentation for a code project, contained in a special branch. The theses only use this branch. Uploading a changed file to github will make the website update as well.</p>

<p>Having the work in progress online is very practical for the teacher, for they can simply click through links instead of having to download code. This does mean in-progress texts are publicly accessible, to mitigate this problem we have shown students how to make sure their site does not get indexed by search engines.</p>
</section>
</section>

<section data-type="sect1">
<h1>Indexation, Syndication</h1>

<p>The one major advantage of data-base driven Content Management Systems is that it is very easy to generate indexes. The same database from which a single page will be fetched, will just as happily generate a listing of pages, sorted and filtered as the designer demands. Because most CMS’s are dynamic, these indexes will be kept up to date automatically: every time the page is rendered, the indexes will be re-generated.</p>

<p>To create indexes from a collection of HTML files is more involved: since there is no neat database model with fields like ‘title’, ‘author’, ‘publishing date’, one will need a way to discover these metadata from the HTML. Creating indexes by hand is one option, but this is error-prone and tedious.</p>

<p>It becomes even more involved when the indexes themselves are part of the pages, like in the submenu of a publication that show links to other pages. If a collaborator on another page changes its title, or a new page gets added, this submenu should be updated in every page—again, tedious and error-prone.</p>

<p>With the theses at the KABK we decided to tackle the first problem. The students do not create actual links in-between the various theses. But a general index of the theses is desirable.</p>

<section data-type="sect2">
<h2>Turning the database inside out</h2>

<p>To create the index, we rely on the metadata provided to social networks and search engines. As practical as the database underlying CMS’s is, it has one big problem: it is inaccesible to the outside world. This has been a concern for companies that are built upon indexing and syndicating the web: search engines like Google, and social networks like Facebook and Twitter. These companies want to be able to either index, relate and preview web pages, and they need data for that.</p>

<p>There has been structured metadata for almost as long as HTML exists. As always, there are conflicting standards. RDF/A, microdata and JSON-LD are now competing. We use a set of metadata that is currently recognised by search engines and social networks. The students implement this metadata in their theses. A script ‘scrapes’ all theses, analyses the metadata, and generates the index.</p>
</section>
</section>

<aside data-type="sidebar">
<h1>Advantages of static sites</h1>

<section data-type="sect2">
<h2>Speed</h2>

<p class="p1"><span style="letter-spacing: 0.01em; line-height: 1.3em;">Before a dynamic web application can serve a page to a user, it will first need to ask information from a database and push this data through a template. HTML can be sent to the user right away. The first web servers like the Apache server made a very simple mapping between urls and folders filled with HTML files, that still holds today. The technology to serve static files has been steadily perfected, and the now popular nGinx server does it even faster than Apache.</span></p>
</section>

<p class="p2">&nbsp;</p>

<section data-type="sect2">
<h2>Security</h2>

<p class="p2"><span style="letter-spacing: 0.01em; line-height: 1.3em;">Since the website does not need to store information in a database, the ‘attack surface’ of a website hosting static files is much smaller. The key here is that the HTML files are generated on a users computer, and uploaded afterwards. A conventional content management system (like Wordpress) needs to expose a kind of editing functionality through a web interface. Hackers can and will attempt to gain access to this web interface. Because the ways in which the authentication system, the interface, and the database work together are quite complicated, new weak spots are discovered in these systems regularly. Popular projects like Wordpress monitor the security threats actively, and release updated versions as new threats are discovered. But this means anyone using such as system needs to be &amp; in keeping them up to date.</span></p>
</section>

<section data-type="sect2">
<h2>Maintainability &amp; costs</h2>

<p class="p1">Like mentioned in the previous point, dynamic websites need to be kept up to date to stay secure. This requires an amount of work that is non-negligible. A static website, one can essentially forget about: as long as the hosting bills are being paid, it should be able to continue functioning.</p>

<p class="p1">Dynamic websites use more complicated hosting setups. They might even require a system known as a ‘VPS’, a virtual private server that can be fully configured: these cost a substantial amount of time to keep running and are expensive. Static websites can be hosted on more simple infrastructures. The cheapest shared web host will support the necessary technology.&nbsp;</p>
</section>

<section data-type="sect2">
<h2>Archivability</h2>

<p class="p1">A static website is essentially the only guaranteed way of archiving a design. If a dynamic website requires its dependent CMS to be kept up to date. This means going along with new versions of the dependent software. The problem is that, as software progresses, it changes, and some functions implemented in the previous versions mutate and disappear. Parts of the website then need to be reimagined, and there is a high chance this has repercussions on the design. Static websites are therefore ideal for sites that need to be archived: their contents needs to remain accessible, but does not need to be updated often.</p>
</section>
</aside>
</section>
